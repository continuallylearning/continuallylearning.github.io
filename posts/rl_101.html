<!DOCTYPE html>
<html lang="en">
<head>
<link rel="stylesheet" href="style.css">
<link href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism.min.css" rel="stylesheet" />
<script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs/components/prism-python.min.js"></script>

  <meta charset="utf-8">
  <title>Reinforcement Learning 101</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" href="../style.css">

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" async></script>
</head>
<body>
  <div class="container">
    <p><a href="../index.html">&larr; Back to home</a></p>

    <article>
      <button id="theme-toggle">🌙 Night Mode</button>
      <h1>Reinforcement Learning 101</h1>
      <!-- TODO: Set time correctly-->
      <!-- TODO: Spellcheck-->
       <!-- TODO: Make homework problems more obvious-->
      <p><time datetime="2025-09-20">September 26, 2025</time></p>

      Reinforcement Learning (RL) is a framework and collection of approaches for creating agents that can interact with environments to achieve goals. 
      
      <!-- TODO: Make jumps to each section here-->
       <!-- Put the name of the section as answer for each question here, encourage people to jump around as needed. -->
      Some natural questions to ask are:
      <ul>
        <li>What is an environment?</li>
        <li>What is an agent?</li>
        <li>What does it mean for an agent to interact with an environment?</li>
        <li>What is a goal?</li>
        <li>What ways can we make an agent achieve a goal by interacting with an environment?</li>
      </ul>

      We will answer all these questions, starting with defining what an environment is.
      
      <h1>Environment is modeled as a Markov Decision Process (MDP)</h1>

      We model an environment as a Markov Decision Process (MDP), which is defined as a 5-tuple $MDP = \{S,A,T,R,\gamma\}$, where 
      
      <ul>
        <li>$S$ is set of states</li>
        <li>$A$ is set of actions</li>
        <li>$T(s^{'}|s,a) = Pr(s^{'} | s, a) : S \times A \rightarrow [0,1]$ is the transition dynamics, and defines the (conditional) probability of transitioning to a state $s^{'} \in S$ conditioned on executing action $a \in A$ from state $s \in S$.</li>
        <li>$R(r | s, a) = Pr(r | s, a) : S \times A \times R \rightarrow [0,1]$ is the reward function, and defines the (conditional) probability of receiving a scalar reward $r$ for taking action $a$ from state $s$.</li>
        <ul>
        <li>When the reward is a deterministic function of the state and action, you may see the reward function instead written not as a conditional probability, but as a function $R(r|s,a) : S \times A \rightarrow R$ that maps state and actions directly to a scalar. Our probabilistic formulation generalizes this setting, and makes certain future definitions clearer as will be seen shortly. </li>
        </ul>
        <li>$\gamma : \mathbb{R}$ is the discount factor, and it determines how much less valuable rewards become when they are received in the future (This will be explained in more detail shortly)</li>
      </ul>


      To understand why an MDP is a useful model of an environment, let's define what an agent is (which is the entity that interacts with the environment).

      <h1>An agent is modeled as a policy</h1>
      A policy $\pi(a|s) = Pr(a|s): S \times A \rightarrow \mathbb{R}$ is the (conditional) probability of an action $a$ from a state $s$. 
      <ul>
        <li>Defining the policy to be deterministic (i.e: agent's behavior isn't stochastic as a function of state) is just a special case of our probabilistic definition.</li>
        </ul>
      
      Intuitively, an agent's behavior is determined by how it acts in different situations. A policy $\pi(a|s)$ determines how an agent acts in different situations, because we can sample actions from the policy given a state $a \sim \pi(*|s)$.
    
    <h2>Policies generate trajectories by interacting with environment</h2>
      
      When an agent (policy) interacts with an environment (MDP) for $T$ timesteps, it generates a trajectory $\tau_T$. More formally, we define a trajectory as a sequence of $T$ steps, where each step contains a state, action, and reward:
  
       $$
      \tau_{T} = \{s_0,a_0,r_0,s_1,a_1,r_1,...,s_{T-1},a_{T-1},r_{T-1}, s_{T}\}
       $$
      <ul>
        <li>This definition of trajectory (where it ends with state) is somewhat arbitrary, this is just consistent with our following definitions.</li>
      </ul>
  
      Take note our definition of a trajectory $\tau_{T}$ has $T+1$ states in it, but only $T$ actions and rewards. Here is why: we can think of the environment at time $t=0$ as being in some start state $s_{0}$. The agent will then sample an action $a_{0} \sim \pi(*|s_0)$, and the environment will produce a reward $r_0$ and next state $s_1$, and we call this a step. So each step generates an action, reward, and (next) state, and so if we take $T$ steps, and we started with an initial state, we get $T+1$ states and $T$ action and rewards.</li>
  

    <h2>Trajectories have a cumulative discounted reward</h2>
    To formalize the idea that some trajectories are "better", we want to be able to associate a scalar number to each trajectory, and "better" trajectories should be assigned higher numbers. The best trajectory has the bigger number! We formalize this by defining a function $G(\tau): \tau_{T} \in \mathbb{T} \rightarrow {\mathbb{R}}$ that maps a trajectory to a single number representing the cumulative discounted rewards:

    $$
    G(\tau_T) = \sum_{t=0}^{T-1} \gamma^{t} \cdot r_{t}
    $$

    <ul>
      <li>When the discount factor $\gamma=1$, then this is just the cumulative rewards of the trajectory. Since $\gamma$ is between $0$ and $1$ and is exponential in time, we are (exponentially) discounting future rewards, which is why we call $G(\tau)$ the cumulative discounted rewards.</li>
    </ul>

    Cumulative discounted rewards $G(\tau)$ are crucial for defining what we mean by goals. Although trajectories $\tau$ that make $G(\tau)$ go up high are "good", we also have to account for how likely the trajectory is.

    <h2>Probability of trajectory depends on MDP and policy</h2>

    We can formalize our aforementioned intuition for how an agent interacts with an environment by defining the probability of a trajectory in a highly factorized way. We won't go into details here (it is best described using graphical models), but our intuition is equivalent to making a bunch of conditional independence assumptions that are implied by our definitions of the transition dynamics, reward function, and policy all being function of the current state (and action). This is what the word <i>Markov</i> in MDP refers to.
    <br><br>
    
    For a specific environment $MDP$ and policy $\pi$, we can define the probability of a trajectory $\tau_{T}$ as:

    $$
    Pr_{\pi,T,R}(\tau_{T}) = Pr(s_0) \cdot \prod_{t=1}^{T} \pi(a_{t-1} | s_{t-1}) \cdot R(r_{t-1}|s_{t-1},a_{t-1}) \cdot T(s_t | s_{t-1}, a_{t-1})
    $$

    <ul>
      <li>We add $\pi$, $T$, and $R$ to the subscript of $Pr(\tau)$ to emphasize that this probability depends on the distributions of the policy, transition dynamics, and reward function, but when that is not relevant, we just refer to it as $Pr(\tau)$.</li>
      <li>$Pr(s_0) = d(s): S \rightarrow [0,1]$ is called the starting state distribution, and is a distribution over starting states (i.e: how likely a state $s$ is to be the state the agent starts in). This (marginal) distribution can be dependent (or independent) of the policy and transition dynamics.</li>
      <li>Homework Problem: Getting the time indices to all line up takes care! Confirm that our definition of $Pr(\tau_{T})$ doesn't have an "index out of bounds" error.</li>
    </ul>

  Now that we have defined that we have defined the (cumulative discounted) value of a trajectory, as well as the probability of a trajectory, we can define a goal via the <i>reward hypothesis</i>

  <h2>Goals are maximizing Cumulative Discounted Expected (CDE) rewards</h2>

  <a href="http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html">To quote Rich Sutton, the reward hypothesis is defined as </a>:

  <!-- TODO: Make this quote stand out more -->
  <br><br>
  "That all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)."
  <br><br>

  We can formalize this statement as following optimization problem: Find the policy $\pi$ from a space of policies $\Pi$ that maximizes the Cumulative Discounted Expected rewards (CDE): 

  \begin{aligned}
  \pi^{*} &= \arg \max_{\pi \in \Pi} \mathbb{E}_{\tau \sim Pr(\tau)}[G(\tau)] \\
  \end{aligned}

  <ul>
    <li>We've done all the work to define all the terms in this optimization problem. Note that $Pr(\tau)$ depends on $\pi$, $T$ and $R$, while $G(\tau)$ just depends on $\gamma$ (Confirm this by looking at the definition of $G(\tau)$ and $Pr(\tau)$ and see what is required to compute them given you have $\tau$ already). To emphasize this, we may write $Pr_{\pi,T,R}(\tau)$ to emphasize the probability depends on the policy, which we leverage shortly. </li>
    <li>We call $\pi^{*}$ the optimal policy, and "solving the RL problem" amounts to finding the optimal policy (or at least getting as "close" to optimal as possible). The optimal policy does not need to be unique (i.e: there may be multiple optimal policies for a single MDP).</li>
  </ul>

  In order to final optimal policies, it helps to understand the CDE rewards of a policy from different states. We formalize this idea with the idea of a value function $V^{\pi}(s)$

  <h2>Value functions represent CDE rewards from a state</h2>
  The value function $V^{\pi}(s)$ is defined as the CDE rewards conditioned on running the policy $\pi$ from a given state $s$:

  $$
  V^{\pi}(s) = \mathbb{E}_{\tau \sim Pr(\tau)}[G(\tau) | s_0 = s]
  $$

  Given a policy $\pi$ and an MDP, estimating $V{\pi}(s)$ is called <i>policy evaluation</i>.
  

  One way to understand why the value function is useful (i.e: why evaluating a policy is useful) is to define our objective above with it:

  \begin{aligned}
  \pi^{*} &= \arg \max_{\pi \in \Pi} \mathbb{E}_{\tau \sim Pr(\tau)}[G(\tau)] \\
  &= \arg \max_{\pi \in \Pi} \mathbb{E}_{s \sim d(*)}[V^{\pi}(s)]
  \end{aligned}
  
  Notice how in the first line, the expectation is over a distributions of trajectories, while in the second line, the expectation is over a distribution of (starting) states. This means that the cumulative discounted expected rewards of a policy (over a distribution of trajectories) is equal to the expected value function of a policy (over the distribution of starting states).


  <!-- Script for dark mode -->
  <script>
    const toggleBtn = document.getElementById('theme-toggle');
    const body = document.body;
  
    // If user has a saved preference, use that
    const savedTheme = localStorage.getItem('theme');
  
    if (savedTheme === 'light') {
      body.classList.remove('dark-mode');
      toggleBtn.textContent = "🌙 Night Mode";
    } else {
      // Default to dark if nothing saved
      body.classList.add('dark-mode');
      toggleBtn.textContent = "☀️ Light Mode";
    }
  
    toggleBtn.addEventListener('click', () => {
      body.classList.toggle('dark-mode');
      const isDark = body.classList.contains('dark-mode');
      toggleBtn.textContent = isDark ? "☀️ Light Mode" : "🌙 Night Mode";
      localStorage.setItem('theme', isDark ? 'dark' : 'light');
    });
  </script>


</body> 


</html>
