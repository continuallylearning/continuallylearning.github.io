<!DOCTYPE html>
<html lang="en">
<head>
<link rel="stylesheet" href="style.css">
<link href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism.min.css" rel="stylesheet" />
<script src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs/components/prism-python.min.js"></script>

  <meta charset="utf-8">
  <title>Reinforcement Learning 101</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" href="../style.css">

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" async></script>
</head>
<body>
  <div class="container">
    <p><a href="../index.html">&larr; Back to home</a></p>

    <article>
      <button id="theme-toggle">üåô Night Mode</button>
      <h1>Reinforcement Learning 101</h1>
      <!-- TODO: Set time correctly-->
      <!-- TODO: Spellcheck-->
       <!-- TODO: Make homework problems more obvious-->
      <p><time datetime="2025-09-20">September 26, 2025</time></p>

      Reinforcement Learning (RL) is a framework and collection of approaches for creating agents that can interact with environments to achieve goals. 
      
      Some natural questions to ask are:
      <ul>
        <li>What is an environment?</li>
        <li>What is an agent?</li>
        <li>What does it mean for an agent to interact with an environment?</li>
        <li>What is a goal?</li>
        <li>What ways can we make an agent achieve a goal by interacting with an environment?</li>
      </ul>

      We will answer all these questions, starting with defining what an environment is.
      
      <h1>Environment is modeled as a Markov Decision Process (MDP)</h1>

      We model an environment as a Markov Decision Process (MDP), which is defined as a 5-tuple $MDP = \{S,A,T,R,\gamma\}$, where 
      
      <ul>
        <li>$S$ is set of states</li>
        <li>$A$ is set of actions</li>
        <li>$T(s^{'}|s,a) = Pr(s^{'} | s, a) : S \times A \rightarrow [0,1]$ is the transition dynamics, and defines the (conditional) probability of transitioning to a state $s^{'} \in S$ conditioned on executing action $a \in A$ from state $s \in S$.</li>
        <li>$R(r | s, a) : S \times A \times R \rightarrow [0,1]$ is the reward function, and defines the (conditional) probability of receiving a scalar reward $r$ for taking action $a$ from state $s$.</li>
        <ul>
        <li>When the reward is a deterministic function of the state and action, you may see the reward function instead written not as a conditional probability, but as a function $R(r|s,a) : S \times A \rightarrow R$ that maps state and actions directly to a scalar. Our probabilistic formulation generalizes this setting, and makes certain future definitions clearer as will be seen shortly. </li>
        </ul>
        <li>$\gamma : \mathbb{R}$ is the discount factor, and it determines how much less valuable rewards become when they are recieved in the future (This will be explained in more detail shortly)</li>
      </ul>


      To understand why an MDP is a useful model of an environment, let's define what an agent is (which is the entity that interacts with the environment).

      <h1>An agent is modeled as a policy</h1>
      A policy $\pi(a|s) = Pr(a|s): S \times A \rightarrow \mathbb{R}$ is the (conditional) probability of an action $a$ from a state $s$. 
      <ul>
        <li>Defining the policy to be deterministic (i.e: agent's behavior isn't stochastic as a function of state) is just a special case of our probabilistic definition.</li>
        </ul>
      
      Intuitively, an agent's behavior is determined by how it acts in different situations. A policy $\pi(a|s)$ determines how an agent acts in different situations, because we can sample actions from the policy given a state $a \sim \pi(*|s)$.
    
    <h2>Trajectories have a cumulative discounted return</h2>
    To formalize the idea that some trajectories are "better", we want to be able to associate a scalar number to each trajectory, and "better" trajectories should be assigned higher numbers. The best trajectory has the bigger number! We formalize this by defining a function $G(\tau): \tau_{T} \in \mathbb{T} \rightarrow {\mathbb{R}}$ that maps a trajectory to a single number representing the cumulative discounted returns:

    $$
    G(\tau_T) = \sum_{t=0}^{T-1} \gamma^{t} \cdot r_{t}
    $$

    <ul>
      <li>When the discount factor $\gamma=1$, then this is just the cumulative rewards of the trajectory. Since $\gamma$ is between $0$ and $1$ and is exponential in time, we are (exponentially) discsounting future rewards, which is why we call $G(\tau)$ the cumulative discounted returns.</li>
    </ul>

    <h2>Policies generate trajectories by interacting with environment</h2>
      
    When an agent (policy) interacts with an environment (MDP) for $T$ timesteps, it generates a trajectory $\tau_T$. More formally, we define a trajectory as a sequence of $T$ steps, where each step contains a state, action, and reward:

     $$
    \tau_{T} = \{s_0,a_0,r_0,s_1,a_1,r_1,...,s_{T-1},a_{T-1},r_{T-1}, s_{T}\}
     $$

    Take note our definition of a trajectory $\tau_{T}$ has $T+1$ states in it, but only $T$ actions and rewards. There is an intuitive and mathematical justification for this. Intuitively, we think of the environment at time $t=0$ to be in some start state $s_{0}$. The agent will then sample an action $a_{1} \sim \pi(*|s_0)$, and the environment will produce a reward $r_1$ and next state $s_1$, and we call this a step. So each step generates an action, reward, and (next) state, and so if we take $T$ steps, and we started with an initial state, we get $T+1$ states and $T$ action and rewards.</li>


    <h2>Probability of trajectory depends on MDP and policy</h2>

    We can formalize our aforementioned intuition for how an agent interacts with an environment by defining the probability of a trajectory in a highly factorized way. We won't go into details here (it is best described using graphical models), but our intuition is equivalent to making a bunch of conditional independence assumptions that are implied by our definitions of the transition dynamics, reward function, and policy all being function of the current state (and action). This is what the word <i>Markov</i> in MDP refers to.
    <br><br>
    
    For a specific environment $MDP$ and policy $\pi$, we can define the probability of a trajectory $\tau_{T}$ as:

    $$
    Pr(\tau_{T}) = d^{\pi}(s_0) \cdot \prod_{t=1}^{T} \pi(a_{t-1} | s_{t-1}) \cdot R(r_{t-1}|s_{t-1},a_{t-1}) \cdot T(s_t | s_{t-1}, a_{t-1})
    $$

    <ul>
      <li>$d^{\pi}(s) = Pr(s_0=s) : S \rightarrow [0,1]$ is called the starting state distriubtion, and is a distriubtion over starting states (i.e: how likely a state $s$ is to be the state the agent starts in). This distriubtion can depend on the policy or be independent of it, which is why $pi$ is in the superscript of $d^{\pi}$.</li>
      <li>Homework Problem: Getting the time indices to all line up takes care! Confirm that our definition of $Pr(\tau_{T})$ doesn't have an "index out of bounds" error.</li>
    </ul>




  <!-- Script for dark mode -->
  <script>
    const toggleBtn = document.getElementById('theme-toggle');
    const body = document.body;
  
    // If user has a saved preference, use that
    const savedTheme = localStorage.getItem('theme');
  
    if (savedTheme === 'light') {
      body.classList.remove('dark-mode');
      toggleBtn.textContent = "üåô Night Mode";
    } else {
      // Default to dark if nothing saved
      body.classList.add('dark-mode');
      toggleBtn.textContent = "‚òÄÔ∏è Light Mode";
    }
  
    toggleBtn.addEventListener('click', () => {
      body.classList.toggle('dark-mode');
      const isDark = body.classList.contains('dark-mode');
      toggleBtn.textContent = isDark ? "‚òÄÔ∏è Light Mode" : "üåô Night Mode";
      localStorage.setItem('theme', isDark ? 'dark' : 'light');
    });
  </script>


</body> 


</html>
